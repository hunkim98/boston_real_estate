{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data/2020'\n",
    "\n",
    "single_family_filename = 'single_family_sales.csv'\n",
    "three_family_filename = 'three_family_sales.csv'\n",
    "condo_filename = 'condo_sales.csv'\n",
    "two_family_filename = 'two_family_sales.csv'\n",
    "\n",
    "single_family_sales = pd.read_csv(folder_path + \"/\" + single_family_filename)\n",
    "three_family_sales = pd.read_csv(folder_path + \"/\" + three_family_filename)\n",
    "condo_sales = pd.read_csv(folder_path + \"/\" + condo_filename)\n",
    "two_family_sales = pd.read_csv(folder_path + \"/\" + two_family_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[575, 604, 815, 4379]\n"
     ]
    }
   ],
   "source": [
    "all_lengths = [len(single_family_sales), len(three_family_sales), len(condo_sales), len(two_family_sales)]\n",
    "all_lengths.sort()\n",
    "\n",
    "print(all_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['parcel', 'street_no', 'street_name', 'unit', 'sale_date', 'sale_price',\n",
      "       'living_area', 'price_per_sf', 'category'],\n",
      "      dtype='object')\n",
      "Index(['parcel', 'street_no', 'street_name', 'unit', 'sale_date', 'sale_price',\n",
      "       'living_area', 'price_per_sf', 'category'],\n",
      "      dtype='object')\n",
      "Index(['parcel', 'street_no', 'street_name', 'unit', 'sale_date', 'sale_price',\n",
      "       'living_area', 'price_per_sf', 'category'],\n",
      "      dtype='object')\n",
      "Index(['parcel', 'street_no', 'street_name', 'unit', 'sale_date', 'sale_price',\n",
      "       'living_area', 'price_per_sf', 'category'],\n",
      "      dtype='object')\n",
      "{'unit', 'street_name', 'sale_date', 'parcel', 'category', 'sale_price', 'street_no', 'price_per_sf', 'living_area'}\n",
      "['unit', 'street_name', 'sale_date', 'parcel', 'category', 'sale_price', 'street_no', 'price_per_sf', 'living_area']\n"
     ]
    }
   ],
   "source": [
    "# add category column\n",
    "single_family_sales['category'] = 'single_family'\n",
    "three_family_sales['category'] = 'three_family'\n",
    "condo_sales['category'] = 'condo'\n",
    "two_family_sales['category'] = 'two_family'\n",
    "\n",
    "# check if there are any differences between the columns \n",
    "print(single_family_sales.columns)\n",
    "print(three_family_sales.columns)\n",
    "print(condo_sales.columns)\n",
    "print(two_family_sales.columns)\n",
    "\n",
    "# print diff columns \n",
    "# find overlapping columns\n",
    "overlapping_columns = set(single_family_sales.columns) & set(three_family_sales.columns) & set(condo_sales.columns) & set(two_family_sales.columns)\n",
    "print(overlapping_columns)\n",
    "\n",
    "overlapping_columns = list(overlapping_columns)\n",
    "# merge the overlapping columns\n",
    "# merged_sales = pd.concat([single_family_sales[overlapping_columns], three_family_sales[overlapping_columns], condo_sales[overlapping_columns], two_family_sales[overlapping_columns]])\n",
    "\n",
    "print(overlapping_columns)\n",
    "# save the merged sales\n",
    "merged_sales = pd.concat([\n",
    "    single_family_sales[overlapping_columns],\n",
    "    three_family_sales[overlapping_columns],\n",
    "    condo_sales[overlapping_columns],\n",
    "    two_family_sales[overlapping_columns]\n",
    "], ignore_index=True)\n",
    "\n",
    "# save\n",
    "merged_sales.to_csv(f\"{folder_path}/merged_sales.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years found in data: ['2019', '2020']\n"
     ]
    }
   ],
   "source": [
    "# check if the merged sales are not the same year\n",
    "\n",
    "# check the sale date and check if it all ends with /2022\n",
    "# This will return True only if ALL dates end with /2022, which is not what we want\n",
    "# We should check if ANY dates end with /2022 to catch mismatched years\n",
    "has_2022 = merged_sales['sale_date'].str.endswith('/2022').any()\n",
    "if has_2022:\n",
    "    print(\"Warning: Found sales from 2022 when all should be from 2023\")\n",
    "    \n",
    "# Let's check what years we actually have\n",
    "years = merged_sales['sale_date'].str.extract(r'/(\\d{4})$')[0].unique()\n",
    "print(\"Years found in data:\", sorted(years))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 2019 sales in current dataset: 662\n",
      "Number of sales in original 2019 file: 5973\n",
      "Warning: Found 40 records from 2019 that were not in the original 2019 file\n",
      "Sample of new records:\n",
      "           parcel  sale_date\n",
      "899  01-00179-000  8/29/2019\n",
      "913  01-04909-000   8/2/2019\n",
      "927  01-04320-001  7/23/2019\n",
      "930  02-02060-000   8/2/2019\n",
      "983  07-02540-001  5/31/2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/50c5sv7n4wsfhhww_wwjz2tr0000gn/T/ipykernel_55192/3227019566.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_2019_merged_sales['unique_id'] = target_2019_merged_sales['parcel'] + '_' + target_2019_merged_sales['sale_date']\n"
     ]
    }
   ],
   "source": [
    "# now since 2020 has both 2019 and 2020, we check if 2019 values are all included in original 2019 files\n",
    "\n",
    "original_2019_merged_sales = pd.read_csv(f\"data/2019/merged_sales.csv\")\n",
    "\n",
    "# extract 2019 from merged_sales\n",
    "target_2019_merged_sales = merged_sales[merged_sales['sale_date'].str.endswith('2019')]\n",
    "\n",
    "print(f\"Number of 2019 sales in current dataset: {len(target_2019_merged_sales)}\")\n",
    "print(f\"Number of sales in original 2019 file: {len(original_2019_merged_sales)}\")\n",
    "\n",
    "# Create unique identifiers combining parcel and sale_date\n",
    "original_2019_merged_sales['unique_id'] = original_2019_merged_sales['parcel'] + '_' + original_2019_merged_sales['sale_date']\n",
    "target_2019_merged_sales['unique_id'] = target_2019_merged_sales['parcel'] + '_' + target_2019_merged_sales['sale_date']\n",
    "\n",
    "# Check for any 2019 records that appear in current dataset but not in original 2019 file\n",
    "new_records = target_2019_merged_sales[~target_2019_merged_sales['unique_id'].isin(original_2019_merged_sales['unique_id'])]\n",
    "if len(new_records) > 0:\n",
    "    print(f\"Warning: Found {len(new_records)} records from 2019 that were not in the original 2019 file\")\n",
    "    print(\"Sample of new records:\")\n",
    "    print(new_records[['parcel', 'sale_date']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "0\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# we should scrutinize the new records and really see if the parcels don't exist in the original 2019 file\n",
    "new_records_parcels = new_records['parcel'].unique()\n",
    "original_2019_merged_sales_parcels = original_2019_merged_sales['parcel'].unique()\n",
    "\n",
    "# find the parcels that are in new_records but not in original_2019_merged_sales\n",
    "new_records_parcels_not_in_original = set(new_records_parcels) - set(original_2019_merged_sales_parcels)\n",
    "\n",
    "print(len(new_records_parcels_not_in_original))\n",
    "print(len(new_records_parcels))\n",
    "\n",
    "# check the overlapping parcels\n",
    "overlapping_parcels = set(new_records_parcels) & set(original_2019_merged_sales_parcels)\n",
    "print(len(overlapping_parcels))\n",
    "print(overlapping_parcels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit</th>\n",
       "      <th>street_name</th>\n",
       "      <th>sale_date</th>\n",
       "      <th>parcel</th>\n",
       "      <th>category</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>street_no</th>\n",
       "      <th>price_per_sf</th>\n",
       "      <th>living_area</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [unit, street_name, sale_date, parcel, category, sale_price, street_no, price_per_sf, living_area, unique_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the overlapping parcels dataframe\n",
    "new_records_parcels_not_in_original_df = new_records[new_records['parcel'].isin(overlapping_parcels)]\n",
    "new_records_parcels_not_in_original_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit</th>\n",
       "      <th>street_name</th>\n",
       "      <th>sale_date</th>\n",
       "      <th>parcel</th>\n",
       "      <th>category</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>street_no</th>\n",
       "      <th>price_per_sf</th>\n",
       "      <th>living_area</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [unit, street_name, sale_date, parcel, category, sale_price, street_no, price_per_sf, living_area, unique_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_2019_merged_sales_parcels_df = original_2019_merged_sales[original_2019_merged_sales['parcel'].isin(overlapping_parcels)]\n",
    "original_2019_merged_sales_parcels_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the new records to the original 2019 merged sales\n",
    "original_2019_merged_sales = pd.concat([original_2019_merged_sales, new_records_parcels_not_in_original_df], ignore_index=True)\n",
    "\n",
    "# remove the unique_id column\n",
    "original_2019_merged_sales = original_2019_merged_sales.drop(columns=['unique_id'])\n",
    "\n",
    "# save the original 2019 merged sales\n",
    "original_2019_merged_sales.to_csv(f\"data/2019/merged_sales.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6372\n",
      "6373\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_sales[\"parcel\"].unique()))\n",
    "print(len(merged_sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
